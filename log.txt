Log 10 5 2017:
    Grab doc_ids and text from authors Newsom, Sisco, Bergus, Rusk, Burdett
    for each author collect works and build an average feature profile from
    80% of their documents, use 20% for training data.
    Features for set:
        
    mysql -u de_reader -p -h history-lab.org --database declassification_frus -e select doc_id from authorship where name=Burdett > ./ID_Burdett
    used for ids of each author

    Draft email to Mosha about data- what feature set want, describe data,
    all data within, date as feature, kinds authorship we consider- for the
    large number of documents that aren't necessarily, draft a few different
    problems that we can look at. Should the data be stripped in any way?

Log 10 12 2017
    Mean doc length based on words:
      Bergus: 745.81
      Newsom: 536.88
      Sisco: 753.34
      Rusk: 533.07
      Burdett: 498.44
    Mean doc length based on characters: 
      Bergus: 4489.61
      Newsom: 3307.22
      Sisco: 4532.28
      Rusk: 3120.32
      Burdett: 3070.55
    Word Count Ranges:
      Bergus: 100 to 4308 with standard deviation 10.78
      Newsom: 87 to 2820 with standard deviation 11.18
      Sisco: 48 to 8038 with standard deviation 13.42
      Rusk: 51 to 2444 with standard deviation 9.50
      Burdett: 120 to 2101 with standard deviation 9.12
    two sample docs to give-
    Can we use date as a feature?
    
    What feature set would work well for the data
    What authorship problems do we have: we have a small number of authors
    with known document counts over 50 - provide that text document, large
    number of authors with fewer, and many documents without an author. We
    want to be able to attribute unknown documents to known authors or
    confidently build sets of documents probably written by the same author.
    Our data is also edited/dictated by secretaries in many cases. Is it
    possible to distinguish between author and editor? Should our data be
    stripped in any way? What feature sets and techniques may prove
    fruitful to test on this corpus? Can we use date as a feature? We have
    documents spanning several decades. Some data are transcripts, how would
    someone writing down another persons words affect their writing style?
    Should we throw out longer texts or are they beneficial? Can we extra
    fields from the text like title and subject, also need to remove all
    candidate docs that have no body or body with transcript. Conversation
    memorandum is probably okay. How should I break up longer texts? We have
    unusual line breaks that are an artifact of digitization process.

Log 10 26 2017:
  started work on scripts to scrub data: remove transcripts, headers, splits
  long documents into 80 word sections.
